# blockchain_paper_daily.py

import arxiv
import time
from datetime import datetime, timedelta
import re
import requests
import json
import os
from typing import List, Dict, Optional
import random  # æ·»åŠ éšæœºæ•°å¯¼å…¥

# é»˜è®¤é…ç½®å€¼
DEFAULT_MODEL_NAME = "qwen-plus"
DEFAULT_OUTPUT_FILENAME = "daily_blockchain_paper.md"
DEFAULT_SCHEDULE_TIME = "09:00"
DEFAULT_GENERATION_URL = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"
DEFAULT_ARXIV_CATEGORIES = ['cs.CR', 'cs.DC', 'cs.NI']
DEFAULT_SEARCH_KEYWORDS = ['blockchain', 'smart contract', 'consensus', 'distributed ledger', 'ethereum', 'bitcoin', 'defi']
DEFAULT_MAX_RESULTS_PER_CATEGORY = 100
DEFAULT_DAYS_TO_LOOK_BACK = 30

# å°è¯•å¯¼å…¥æœ¬åœ°é…ç½®æ–‡ä»¶
try:
    from config import (
        DASHSCOPE_API_KEY as LOCAL_API_KEY,
        MODEL_NAME as LOCAL_MODEL_NAME,
        OUTPUT_FILENAME as LOCAL_OUTPUT_FILENAME,
        SCHEDULE_TIME as LOCAL_SCHEDULE_TIME,
        GENERATION_URL as LOCAL_GENERATION_URL,
        ARXIV_CATEGORIES as LOCAL_ARXIV_CATEGORIES,
        SEARCH_KEYWORDS as LOCAL_SEARCH_KEYWORDS,
        MAX_RESULTS_PER_CATEGORY as LOCAL_MAX_RESULTS_PER_CATEGORY,
        DAYS_TO_LOOK_BACK as LOCAL_DAYS_TO_LOOK_BACK
    )
    DASHSCOPE_API_KEY = LOCAL_API_KEY
    MODEL_NAME = LOCAL_MODEL_NAME if LOCAL_MODEL_NAME else DEFAULT_MODEL_NAME
    OUTPUT_FILENAME = LOCAL_OUTPUT_FILENAME if LOCAL_OUTPUT_FILENAME else DEFAULT_OUTPUT_FILENAME
    SCHEDULE_TIME = LOCAL_SCHEDULE_TIME if LOCAL_SCHEDULE_TIME else DEFAULT_SCHEDULE_TIME
    GENERATION_URL = LOCAL_GENERATION_URL if LOCAL_GENERATION_URL else DEFAULT_GENERATION_URL
    ARXIV_CATEGORIES = LOCAL_ARXIV_CATEGORIES if LOCAL_ARXIV_CATEGORIES else DEFAULT_ARXIV_CATEGORIES
    SEARCH_KEYWORDS = LOCAL_SEARCH_KEYWORDS if LOCAL_SEARCH_KEYWORDS else DEFAULT_SEARCH_KEYWORDS
    MAX_RESULTS_PER_CATEGORY = LOCAL_MAX_RESULTS_PER_CATEGORY if LOCAL_MAX_RESULTS_PER_CATEGORY else DEFAULT_MAX_RESULTS_PER_CATEGORY
    DAYS_TO_LOOK_BACK = LOCAL_DAYS_TO_LOOK_BACK if LOCAL_DAYS_TO_LOOK_BACK else DEFAULT_DAYS_TO_LOOK_BACK
except ImportError:
    # å¦‚æœæ²¡æœ‰æœ¬åœ°é…ç½®æ–‡ä»¶ï¼Œåˆ™ä»ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼è·å–
    DASHSCOPE_API_KEY = os.environ.get("DASHSCOPE_API_KEY", "YOUR_DASHSCOPE_API_KEY_HERE")
    MODEL_NAME = os.environ.get("MODEL_NAME", DEFAULT_MODEL_NAME)
    OUTPUT_FILENAME = os.environ.get("OUTPUT_FILENAME", DEFAULT_OUTPUT_FILENAME)
    SCHEDULE_TIME = os.environ.get("SCHEDULE_TIME", DEFAULT_SCHEDULE_TIME)
    GENERATION_URL = os.environ.get("GENERATION_URL", DEFAULT_GENERATION_URL)
    ARXIV_CATEGORIES = os.environ.get("ARXIV_CATEGORIES", DEFAULT_ARXIV_CATEGORIES)
    SEARCH_KEYWORDS = os.environ.get("SEARCH_KEYWORDS", DEFAULT_SEARCH_KEYWORDS)
    MAX_RESULTS_PER_CATEGORY = os.environ.get("MAX_RESULTS_PER_CATEGORY", DEFAULT_MAX_RESULTS_PER_CATEGORY)
    DAYS_TO_LOOK_BACK = os.environ.get("DAYS_TO_LOOK_BACK", DEFAULT_DAYS_TO_LOOK_BACK)

# -------------------------------
# é…ç½®åŒºåŸŸ
# -------------------------------

# 1. DashScope API Key (å·²ä»config.pyæˆ–ç¯å¢ƒå˜é‡å¯¼å…¥)

# 2. è®¾ç½®æœç´¢å‚æ•° (å·²ä»config.pyæˆ–ç¯å¢ƒå˜é‡å¯¼å…¥)

# 3. CCF-Aç±»åŒºå—é“¾ç›¸å…³ä¼šè®®å’ŒæœŸåˆŠåˆ—è¡¨
CCF_A_VENUES = [
    # ç½‘ç»œä¸ä¿¡æ¯å®‰å…¨é¢†åŸŸ
    'CCS',           # ACM Conference on Computer and Communications Security
    'CRYPTO',        # International Association for Cryptologic Research
    'EUROCRYPT',     # European Cryptologic Conference
    'S&P',           # IEEE Symposium on Security and Privacy
    'USENIX Security', # USENIX Security Symposium
    
    # è®¡ç®—æœºä½“ç³»ç»“æ„/é«˜æ€§èƒ½è®¡ç®—/å­˜å‚¨ç³»ç»Ÿé¢†åŸŸ
    'ASPLOS',        # ACM International Conference on Architectural Support for Programming Languages and Operating Systems
    'ISCA',          # ACM/IEEE International Symposium on Computer Architecture
    'MICRO',         # IEEE/ACM International Symposium on Microarchitecture
    'HPCA',          # IEEE International Symposium on High-Performance Computer Architecture
    
    # è®¡ç®—æœºç½‘ç»œé¢†åŸŸ
    'SIGCOMM',       # ACM International Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication
    'NSDI',          # USENIX Symposium on Networked Systems Design and Implementation
    
    # æ•°æ®åº“/æ•°æ®æŒ–æ˜/å†…å®¹æ£€ç´¢é¢†åŸŸ
    'SIGMOD',        # ACM International Conference on Management of Data
    'VLDB',          # International Conference on Very Large Data Bases
    'ICDE',          # IEEE International Conference on Data Engineering
    
    # è½¯ä»¶å·¥ç¨‹/ç³»ç»Ÿè½¯ä»¶/ç¨‹åºè®¾è®¡è¯­è¨€é¢†åŸŸ
    'ICSE',          # International Conference on Software Engineering
    'ESEC/FSE',      # European Software Engineering Conference / Foundations of Software Engineering
    'ASE',           # International Conference on Automated Software Engineering
    'ISSTA',         # International Symposium on Software Testing and Analysis
    
    # äººå·¥æ™ºèƒ½é¢†åŸŸ
    'AAAI',          # Association for the Advancement of Artificial Intelligence
    'IJCAI',         # International Joint Conference on Artificial Intelligence
    'ICML',          # International Conference on Machine Learning
    'NeurIPS',       # Conference on Neural Information Processing Systems
]

# 4. æ¨¡å‹é…ç½® (å·²ä»config.pyæˆ–ç¯å¢ƒå˜é‡å¯¼å…¥)

# 5. è¾“å‡ºæ–‡ä»¶ (å·²ä»config.pyæˆ–ç¯å¢ƒå˜é‡å¯¼å…¥)

# 6. å®šæ—¶ä»»åŠ¡é…ç½® (å·²ä»config.pyæˆ–ç¯å¢ƒå˜é‡å¯¼å…¥)


# -------------------------------
# è¾…åŠ©å‡½æ•°
# -------------------------------

def contains_keywords(text: str, keywords: List[str]) -> bool:
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä»»ä¸€å…³é”®è¯"""
    text_lower = text.lower()
    return any(kw.lower() in text_lower for kw in keywords)

def call_qwen(prompt: str) -> Optional[str]:
    """è°ƒç”¨é€šä¹‰åƒé—® API"""
    if DASHSCOPE_API_KEY == "YOUR_DASHSCOPE_API_KEY_HERE" or DASHSCOPE_API_KEY == "your-actual-api-key-here":
        print("[WARN] æœªé…ç½® DashScope API Keyï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå“åº”")
        # æ¨¡æ‹ŸAPIå“åº”
        time.sleep(1)
        return "æ˜¯"
    
    headers = {
        "Authorization": f"Bearer {DASHSCOPE_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": MODEL_NAME,
        "input": {
            "messages": [
                {"role": "user", "content": prompt}
            ]
        },
        "parameters": {
            "temperature": 0.1,
            "top_p": 0.9,
            "result_format": "message"
        }
    }

    try:
        response = requests.post(GENERATION_URL, headers=headers, data=json.dumps(payload))
        response.raise_for_status()
        result = response.json()
        return result['output']['choices'][0]['message']['content']
    except Exception as e:
        print(f"[ERROR] è°ƒç”¨å¤§æ¨¡å‹å¤±è´¥: {e}")
        return None

def is_blockchain_related(title: str, abstract: str) -> bool:
    """ä½¿ç”¨å¤§æ¨¡å‹åˆ¤æ–­è®ºæ–‡æ˜¯å¦ä¸åŒºå—é“¾ç›¸å…³"""
    prompt = f"""
ä½ æ˜¯ä¸€ä½è®¡ç®—æœºç§‘å­¦é¢†åŸŸçš„ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹è®ºæ–‡ä¿¡æ¯ï¼Œåˆ¤æ–­å…¶ç ”ç©¶å†…å®¹æ˜¯å¦ä¸»è¦å±äº"åŒºå—é“¾"æˆ–"åˆ†å¸ƒå¼è´¦æœ¬æŠ€æœ¯"é¢†åŸŸã€‚
è¿™åŒ…æ‹¬ä½†ä¸é™äºï¼šå…±è¯†ç®—æ³•ã€æ™ºèƒ½åˆçº¦ã€å¯†ç å­¦åè®®ã€å»ä¸­å¿ƒåŒ–åº”ç”¨ã€Layer2æ‰©å®¹æ–¹æ¡ˆã€è·¨é“¾æŠ€æœ¯ç­‰ã€‚

è®ºæ–‡æ ‡é¢˜ï¼š{title}
æ‘˜è¦ï¼š{abstract}

è¯·ä»…å›ç­”"æ˜¯"æˆ–"å¦"ã€‚ä¸è¦è§£é‡ŠåŸå› ã€‚
""".strip()

    answer = call_qwen(prompt)
    if not answer:
        return False
    
    # ç®€å•å¤„ç†ï¼Œæå–ç¬¬ä¸€ä¸ªè¯
    first_word = answer.strip().split()[0].lower() if answer.strip() else ""
    return first_word in ["æ˜¯", "yes", "true", "âœ…"]

def is_ccf_a_venue(venue: str) -> bool:
    """åˆ¤æ–­å‘è¡¨ venue æ˜¯å¦ä¸º CCF-A ç±»"""
    venue_lower = venue.lower()
    for ccf_a in CCF_A_VENUES:
        if ccf_a.lower() in venue_lower:
            return True
    return False

def generate_summary_and_insights(title: str, abstract: str, entry_id: str) -> Dict:
    """ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆä¸­æ–‡æ‘˜è¦å’Œæ ¸å¿ƒäº®ç‚¹"""
    if DASHSCOPE_API_KEY == "YOUR_DASHSCOPE_API_KEY_HERE" or DASHSCOPE_API_KEY == "your-actual-api-key-here":
        print("[WARN] æœªé…ç½® DashScope API Keyï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå“åº”")
        # æ¨¡æ‹ŸAPIå“åº”
        time.sleep(1)
        return {
            "summary": "è¿™æ˜¯æ¨¡æ‹Ÿçš„è®ºæ–‡æ‘˜è¦å†…å®¹ã€‚åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™é‡Œä¼šæ˜¯é€šè¿‡AIæ¨¡å‹ç”Ÿæˆçš„è¯¦ç»†æ‘˜è¦ã€‚",
            "insights": ["æ¨¡æ‹Ÿè¦ç‚¹1", "æ¨¡æ‹Ÿè¦ç‚¹2", "æ¨¡æ‹Ÿè¦ç‚¹3"],
            "recommendation": "è¿™æ˜¯æ¨¡æ‹Ÿçš„æ¨èç†ç”±ã€‚"
        }
    
    prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç§‘ç ”å†…å®¹è§£è¯»åŠ©æ‰‹ï¼Œå°¤å…¶æ“…é•¿å°†å¤æ‚çš„è®¡ç®—æœºç§‘å­¦ç ”ç©¶è½¬åŒ–ä¸ºé€šä¿—æ˜“æ‡‚çš„è¯­è¨€ã€‚

è¯·é˜…è¯»ä»¥ä¸‹è®ºæ–‡ä¿¡æ¯ï¼Œå¹¶å®Œæˆä¸‹åˆ—ä»»åŠ¡ï¼š

1. **æ’°å†™æ‘˜è¦**ï¼šç”¨ä¸€æ®µç®€æ´çš„ä¸­æ–‡ï¼ˆçº¦100-150å­—ï¼‰ï¼Œæ¦‚è¿°è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³ã€è§£å†³çš„é—®é¢˜ä»¥åŠå–å¾—çš„ä¸»è¦æˆæœã€‚è¦æ±‚è¯­è¨€ç”ŸåŠ¨ï¼Œèƒ½è®©éä¸“ä¸šè¯»è€…å¤§è‡´äº†è§£è¿™é¡¹å·¥ä½œçš„æ„ä¹‰ã€‚
2. **æç‚¼è¦ç‚¹**ï¼šåˆ—å‡ºä¸‰ä¸ªæœ€é‡è¦çš„æŠ€æœ¯è´¡çŒ®æˆ–å‘ç°ï¼Œæ¯ä¸€ç‚¹æ§åˆ¶åœ¨20å­—ä»¥å†…ã€‚
3. **æ¨èç†ç”±**ï¼šå†™ä¸€å¥è¯è¯´æ˜ä¸ºä»€ä¹ˆè¿™ç¯‡æ–‡ç« å€¼å¾—å…³æ³¨ã€‚

è®ºæ–‡æ ‡é¢˜ï¼š{title}
æ‘˜è¦åŸæ–‡ï¼š{abstract}
è®ºæ–‡é“¾æ¥ï¼š{entry_id}

è¯·ä¸¥æ ¼æŒ‰ç…§å¦‚ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸é™„åŠ å…¶ä»–æ–‡å­—ï¼š
{{
  "summary": "è¿™é‡Œæ˜¯ä½ çš„ä¸­æ–‡æ‘˜è¦...",
  "insights": ["è¦ç‚¹1...", "è¦ç‚¹2...", "è¦ç‚¹3..."],
  "recommendation": "è¿™å¥è¯åº”èƒ½æ¿€å‘è¯»è€…å…´è¶£..."
}}
""".strip()

    raw_response = call_qwen(prompt)
    if not raw_response:
        return {
            "summary": "æœªèƒ½ç”Ÿæˆæ‘˜è¦ã€‚",
            "insights": ["-", "-", "-"],
            "recommendation": "æš‚æ— æ¨èè¯­ã€‚"
        }
    
    try:
        # å°è¯•è§£æ JSON
        parsed_data = json.loads(raw_response)
        return {
            "summary": parsed_data.get("summary", "æœªæä¾›æ‘˜è¦"),
            "insights": parsed_data.get("insights", ["-", "-", "-"]),
            "recommendation": parsed_data.get("recommendation", "æš‚æ— æ¨èè¯­")
        }
    except json.JSONDecodeError:
        print("[WARN] å¤§æ¨¡å‹è¿”å›å†…å®¹æ— æ³•è§£æä¸ºJSONï¼Œä½¿ç”¨é»˜è®¤å€¼ã€‚")
        return {
            "summary": "æ‘˜è¦ç”Ÿæˆå‡ºé”™ã€‚",
            "insights": ["-", "-", "-"],
            "recommendation": "æš‚æ— æœ‰æ•ˆæ¨èè¯­ã€‚"
        }


def get_recent_candidate_papers() -> List[Dict]:
    """è·å–æœ€è¿‘çš„å€™é€‰è®ºæ–‡åˆ—è¡¨"""
    client = arxiv.Client()
    candidates = []
    
    # è®¡ç®—æœç´¢æ—¶é—´èŒƒå›´
    end_date = datetime.now()
    start_date = end_date - timedelta(days=DAYS_TO_LOOK_BACK)
    
    # ç»Ÿä¸€æ—¶åŒºå¤„ç†
    import pytz
    if end_date.tzinfo is None:
        # å¦‚æœæ˜¯naive datetimeï¼Œæ·»åŠ æœ¬åœ°æ—¶åŒºä¿¡æ¯
        local_tz = pytz.timezone('UTC')  # ä½¿ç”¨UTCæ—¶åŒº
        end_date = local_tz.localize(end_date)
        start_date = local_tz.localize(start_date)
    
    try:
        # æŒ‰å…³é”®è¯æœç´¢
        for keyword in SEARCH_KEYWORDS:
            print(f"[INFO] æ­£åœ¨æœç´¢å…³é”®è¯ '{keyword}' ...")
            
            # æ„é€ æœç´¢å…³é”®è¯
            search = arxiv.Search(
                query=f"all:{keyword}",
                max_results=MAX_RESULTS_PER_CATEGORY,
                sort_by=arxiv.SortCriterion.SubmittedDate,
                sort_order=arxiv.SortOrder.Descending
            )
            
            try:
                results_batch = list(client.results(search))
                print(f"[INFO] å…³é”®è¯ '{keyword}' æ‰¾åˆ° {len(results_batch)} ç¯‡è®ºæ–‡")
                
                for paper in results_batch:
                    # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
                    paper_date = paper.published
                    # ç»Ÿä¸€æ—¶åŒºå¤„ç†
                    if paper_date.tzinfo is None and start_date.tzinfo is not None:
                        # å¦‚æœè®ºæ–‡æ—¥æœŸæ˜¯naive datetimeï¼Œä½†æˆ‘ä»¬çš„æ—¥æœŸæ˜¯aware datetime
                        paper_date = pytz.UTC.localize(paper_date)
                    elif paper_date.tzinfo is not None and start_date.tzinfo is None:
                        # å¦‚æœè®ºæ–‡æ—¥æœŸæ˜¯aware datetimeï¼Œä½†æˆ‘ä»¬çš„æ—¥æœŸæ˜¯naive datetime
                        paper_date = paper_date.replace(tzinfo=None)
                        start_date = start_date.replace(tzinfo=None)
                        end_date = end_date.replace(tzinfo=None)
                        
                    if start_date <= paper_date <= end_date:
                        candidates.append({
                            "title": paper.title,
                            "summary": paper.summary,
                            "authors": [author.name for author in paper.authors],
                            "link": paper.entry_id,
                            "published": paper.published,
                            "comment": getattr(paper, 'comment', '')
                        })
            except Exception as e:
                print(f"[WARN] æœç´¢å…³é”®è¯ '{keyword}' æ—¶å‡ºé”™: {e}")
                continue
                
        print(f"[INFO] æ€»å…±ç­›é€‰å‡º {len(candidates)} ç¯‡å€™é€‰è®ºæ–‡")
        return candidates
        
    except Exception as e:
        print(f"[ERROR] è·å–è®ºæ–‡æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}")
        print("[INFO] å°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º")
        # æä¾›ä¸€äº›ä¸åŒçš„æ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•
        mock_papers = [
            {
                "title": "LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0",
                "summary": "Web 3.0è®©æ¯ä¸ªäººéƒ½èƒ½æ‹¥æœ‰å’Œèµšé’±è‡ªå·±çš„å†…å®¹ï¼Œä½†ä¹Ÿå¼•æ¥å·æ‡’åˆ·ä½è´¨å†…å®¹éª—å¥–åŠ±çš„ç”¨æˆ·ã€‚æœ¬æ–‡æå‡ºLMM-Incentiveï¼Œç”¨å¼ºå¤§çš„å¤šæ¨¡æ€AIæ¨¡å‹å½“'è£åˆ¤'ï¼Œç»“åˆæ™ºèƒ½åˆçº¦å’Œå¼ºåŒ–å­¦ä¹ ï¼Œè®¾è®¡å‡ºèƒ½è‡ªåŠ¨æ¿€åŠ±é«˜è´¨é‡åˆ›ä½œçš„å¥–åŠ±æœºåˆ¶ï¼Œæœ‰æ•ˆé˜²æ­¢ä½œå¼Šï¼Œæå‡å¹³å°å…¬å¹³ä¸æ•ˆç‡ã€‚",
                "authors": ["Jinbo Wen", "Jiawen Kang", "Linfeng Zhang", "Xiaoying Tang", "Jianhang Tang", "Yang Zhang", "Zhaohui Yang", "Dusit Niyato"],
                "link": "http://arxiv.org/abs/2510.04765v1",
                "published": datetime.now() - timedelta(days=1),
                "comment": ""
            },
            {
                "title": "ConsensusNet: A Novel High-Throughput Consensus Algorithm for Blockchain Networks",
                "summary": "åŒºå—é“¾ç½‘ç»œçš„å…±è¯†ç®—æ³•ç›´æ¥å½±å“ç³»ç»Ÿçš„ååé‡å’Œå®‰å…¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºConsensusNetçš„æ–°å…±è¯†ç®—æ³•ï¼Œç»“åˆäº†æ‹œå åº­å®¹é”™å’Œæƒç›Šè¯æ˜çš„ä¼˜ç‚¹ï¼Œæ˜¾è‘—æé«˜äº†äº¤æ˜“å¤„ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„å®‰å…¨æ€§ã€‚",
                "authors": ["Alice Johnson", "Bob Smith", "Charlie Brown", "Diana Lee"],
                "link": "http://arxiv.org/abs/2510.04766v1",
                "published": datetime.now() - timedelta(days=2),
                "comment": ""
            },
            {
                "title": "Privacy-Preserving Smart Contracts with zk-SNARKs Integration",
                "summary": "æ™ºèƒ½åˆçº¦çš„é€æ˜æ€§è™½ç„¶å¸¦æ¥äº†ä¿¡ä»»ï¼Œä½†ä¹Ÿæš´éœ²äº†ç”¨æˆ·çš„éšç§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºzk-SNARKsçš„éšç§ä¿æŠ¤æ™ºèƒ½åˆçº¦æ¡†æ¶ï¼Œåœ¨ä¿è¯åˆçº¦æ­£ç¡®æ‰§è¡Œçš„åŒæ—¶ï¼Œæœ‰æ•ˆéšè—äº†äº¤æ˜“çš„æ•æ„Ÿä¿¡æ¯ã€‚",
                "authors": ["Eva Martinez", "Frank Wilson", "Grace Davis", "Henry Garcia"],
                "link": "http://arxiv.org/abs/2510.04767v1",
                "published": datetime.now() - timedelta(days=3),
                "comment": ""
            },
            {
                "title": "Cross-chain Atomic Swaps with Game-Theoretic Security Guarantees",
                "summary": "è·¨é“¾åŸå­äº¤æ¢æ˜¯å®ç°ä¸åŒåŒºå—é“¾é—´ä»·å€¼è½¬ç§»çš„å…³é”®æŠ€æœ¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å…·æœ‰åšå¼ˆè®ºå®‰å…¨ä¿è¯çš„æ–°å‹è·¨é“¾äº¤æ¢åè®®ï¼Œé€šè¿‡ç»æµæ¿€åŠ±æœºåˆ¶ç¡®ä¿äº¤æ¢è¿‡ç¨‹çš„å®‰å…¨æ€§å’Œå…¬å¹³æ€§ã€‚",
                "authors": ["Ivy Rodriguez", "Jack Anderson", "Kate Thomas", "Leo Jackson"],
                "link": "http://arxiv.org/abs/2510.04768v1",
                "published": datetime.now() - timedelta(days=4),
                "comment": ""
            },
            {
                "title": "Decentralized Identity Verification using Blockchain and Biometrics",
                "summary": "å»ä¸­å¿ƒåŒ–èº«ä»½éªŒè¯æ˜¯Web 3.0çš„é‡è¦åŸºç¡€è®¾æ–½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆåŒºå—é“¾å’Œç”Ÿç‰©è¯†åˆ«æŠ€æœ¯çš„å»ä¸­å¿ƒåŒ–èº«ä»½éªŒè¯ç³»ç»Ÿï¼Œæ—¢ä¿è¯äº†èº«ä»½çš„å”¯ä¸€æ€§ï¼Œåˆä¿æŠ¤äº†ç”¨æˆ·çš„éšç§æ•°æ®ã€‚",
                "authors": ["Mia White", "Noah Harris", "Olivia Martin", "Peter Thompson"],
                "link": "http://arxiv.org/abs/2510.04769v1",
                "published": datetime.now() - timedelta(days=5),
                "comment": ""
            },
            {
                "title": "Energy-Efficient Mining with Renewable Energy Certificates on Blockchain",
                "summary": "åŒºå—é“¾æŒ–çŸ¿çš„é«˜èƒ½è€—é—®é¢˜å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆå¯å†ç”Ÿèƒ½æºè¯ä¹¦çš„èŠ‚èƒ½æŒ–çŸ¿æœºåˆ¶ï¼Œé€šè¿‡ç»æµæ¿€åŠ±å¼•å¯¼çŸ¿å·¥ä½¿ç”¨æ¸…æ´èƒ½æºï¼Œå®ç°å¯æŒç»­å‘å±•ã€‚",
                "authors": ["Quinn Moore", "Rachel Taylor", "Steve Allen", "Tina Young"],
                "link": "http://arxiv.org/abs/2510.04770v1",
                "published": datetime.now() - timedelta(days=6),
                "comment": ""
            },
            {
                "title": "Scalable Layer-2 Solutions with Optimistic Rollups and Fraud Proofs",
                "summary": "Layer-2æ‰©å®¹æ–¹æ¡ˆæ˜¯è§£å†³åŒºå—é“¾æ€§èƒ½ç“¶é¢ˆçš„é‡è¦æ–¹å‘ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆä¹è§‚æ±‡æ€»å’Œæ¬ºè¯ˆè¯æ˜çš„æ–°å‹Layer-2æ¶æ„ï¼Œåœ¨æé«˜äº¤æ˜“å¤„ç†é€Ÿåº¦çš„åŒæ—¶ï¼Œç¡®ä¿äº†èµ„é‡‘çš„å®‰å…¨æ€§ã€‚",
                "authors": ["Uma Scott", "Victor King", "Wendy Wright", "Xavier Hill"],
                "link": "http://arxiv.org/abs/2510.04771v1",
                "published": datetime.now() - timedelta(days=7),
                "comment": ""
            },
            {
                "title": "Quantum-Resistant Cryptographic Algorithms for Future Blockchains",
                "summary": "éšç€é‡å­è®¡ç®—çš„å‘å±•ï¼Œä¼ ç»Ÿå¯†ç å­¦é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡è®¾è®¡äº†ä¸€å¥—æŠ—é‡å­è®¡ç®—çš„å¯†ç å­¦ç®—æ³•ï¼Œå¹¶æ¢è®¨äº†å…¶åœ¨ä¸‹ä¸€ä»£åŒºå—é“¾ä¸­çš„åº”ç”¨ï¼Œä¸ºç³»ç»Ÿçš„é•¿æœŸå®‰å…¨æ€§æä¾›ä¿éšœã€‚",
                "authors": ["Yara Green", "Zack Baker", "Amy Adams", "Ben Clark"],
                "link": "http://arxiv.org/abs/2510.04772v1",
                "published": datetime.now() - timedelta(days=8),
                "comment": ""
            },
            {
                "title": "Machine Learning-Based Anomaly Detection in Blockchain Networks",
                "summary": "åŒºå—é“¾ç½‘ç»œä¸­çš„å¼‚å¸¸è¡Œä¸ºæ£€æµ‹å¯¹ç³»ç»Ÿå®‰å…¨è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæœºå™¨å­¦ä¹ çš„å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œèƒ½å¤Ÿå®æ—¶è¯†åˆ«å’Œé¢„è­¦æ½œåœ¨çš„å®‰å…¨å¨èƒï¼Œæé«˜ç³»ç»Ÿçš„é²æ£’æ€§ã€‚",
                "authors": ["Cindy Lewis", "Dan Walker", "Ella Hall", "Fred Allen"],
                "link": "http://arxiv.org/abs/2510.04773v1",
                "published": datetime.now() - timedelta(days=9),
                "comment": ""
            },
            {
                "title": "Tokenomics Design for Sustainable Decentralized Autonomous Organizations",
                "summary": "ä»£å¸ç»æµå­¦è®¾è®¡ç›´æ¥å½±å“å»ä¸­å¿ƒåŒ–è‡ªæ²»ç»„ç»‡(DAO)çš„å¯æŒç»­æ€§ã€‚æœ¬æ–‡é€šè¿‡åšå¼ˆè®ºå’Œæ§åˆ¶è®ºçš„æ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹ä»£å¸ç»æµæ¨¡å‹ï¼Œæœ‰æ•ˆæ¿€åŠ±é•¿æœŸå‚ä¸å¹¶é˜²æ­¢æ¶æ„è¡Œä¸ºã€‚",
                "authors": ["Gina Young", "Harry King", "Iris Wright", "Jack Lopez"],
                "link": "http://arxiv.org/abs/2510.04774v1",
                "published": datetime.now() - timedelta(days=10),
                "comment": ""
            }
        ]
        return mock_papers


def format_output(paper_info: Dict) -> str:
    """æ ¼å¼åŒ–æœ€ç»ˆè¾“å‡ºå†…å®¹"""
    # æ„å»ºåŸºç¡€æ¨¡æ¿
    template = f"""# ğŸ“š ArXiv åŒºå—é“¾è®ºæ–‡æ—¥æŠ¥ ({datetime.now().strftime('%Y-%m-%d')})

> ğŸ” æ¥æºï¼šè‡ªåŠ¨æŠ“å– ArXiv æœ€æ–°è®ºæ–‡å¹¶é€šè¿‡é€šä¹‰åƒé—®ç²¾é€‰

---

## ğŸ“˜ è®ºæ–‡æ ‡é¢˜
[{paper_info['title']}]({paper_info['link']})

## ğŸ‘¥ ä½œè€…
{', '.join(paper_info['authors'])}
"""
    
    # æ·»åŠ ç¬¬ä¸€å•ä½ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'affiliation' in paper_info and paper_info['affiliation']:
        template += f"\n\nğŸ¢ ç¬¬ä¸€å•ä½\n{paper_info['affiliation']}"
    
    # æ·»åŠ å‘è¡¨ä¿¡æ¯
    template += f"""

## ğŸ—‚ï¸ å‘è¡¨ä¿¡æ¯
ArXiv é¢„å°æœ¬"""
    
    if 'published' in paper_info:
        template += f" â€¢ æäº¤æ—¥æœŸ: {paper_info['published'].strftime('%Y-%m-%d')}"
    
    # æ·»åŠ å†…å®¹é€Ÿè§ˆ
    template += f"""

## ğŸ§¾ å†…å®¹é€Ÿè§ˆ

### ğŸ’¡ æ ¸å¿ƒæ‘˜è¦
{paper_info['summary']}

### â­ å…³é”®çœ‹ç‚¹
"""
    for insight in paper_info['insights']:
        template += f"- {insight}\n"

    template += f"\n## ğŸ¯ æ¨èç†ç”±\n{paper_info['recommendation']}\n\n"

    template += "---\n*ğŸ¤– ç”± AI è‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒ*\n"
    return template


def format_xiaohongshu_output(paper_info: Dict) -> str:
    """ç”Ÿæˆå°çº¢ä¹¦é£æ ¼çš„è¾“å‡ºå†…å®¹"""
    # ç”ŸæˆåŠ¨æ€è¯é¢˜æ ‡ç­¾
    hashtags = generate_xiaohongshu_hashtags(paper_info)
    
    template = f"""
ğŸ“– è®ºæ–‡æ ‡é¢˜ï¼š{paper_info['title']}

ğŸ‘¥ ä½œè€…ä¿¡æ¯ï¼š{', '.join(paper_info['authors'])}

ğŸ“… å‘å¸ƒæ—¥æœŸï¼š{paper_info['published'].strftime('%Yå¹´%mæœˆ%dæ—¥')}

ğŸ” è®ºæ–‡æ‘˜è¦ï¼š
{paper_info['summary']}

â­ å…³é”®çœ‹ç‚¹ï¼š"""
    
    for i, insight in enumerate(paper_info['insights'], 1):
        template += f"\n{i}. {insight}"

    template += f"""

ğŸ¯ æ¨èç†ç”±ï¼š
{paper_info['recommendation']}

ğŸ”— åŸæ–‡é“¾æ¥ï¼š{paper_info['link']}

ğŸ¤– æœ¬å†…å®¹ç”±AIç”Ÿæˆï¼Œè®¿é—®é¡¹ç›®äº†è§£æ›´å¤šï¼š
https://github.com/jialinpeng/ai-blockchain-paper-share

{hashtags}
"""
    return template


def generate_xiaohongshu_hashtags(paper_info: Dict) -> str:
    """æ ¹æ®è®ºæ–‡å†…å®¹å’Œæœç´¢å…³é”®è¯ç”Ÿæˆå°çº¢ä¹¦è¯é¢˜æ ‡ç­¾"""
    title = paper_info['title'].lower()
    summary = paper_info['summary'].lower()
    
    # å®šä¹‰ä¸€äº›å¸¸è§çš„åŒºå—é“¾ç›¸å…³ä¸»é¢˜è¯
    topics = {
        'åŒºå—é“¾': ['blockchain', 'åŒºå—é“¾'],
        'å…±è¯†æœºåˆ¶': ['consensus', 'å…±è¯†', 'bft', 'æ‹œå åº­', 'pbft'],
        'æ™ºèƒ½åˆçº¦': ['smart contract', 'æ™ºèƒ½åˆçº¦', 'solidity'],
        'ç½‘ç»œå®‰å…¨': ['security', 'attack', 'å®‰å…¨', 'æ”»å‡»', 'é˜²æŠ¤'],
        'éšç§ä¿æŠ¤': ['privacy', 'åŒ¿å', 'é›¶çŸ¥è¯†', 'zk', 'private'],
        'æ€§èƒ½ä¼˜åŒ–': ['performance', 'scalability', 'sharding', 'åˆ†ç‰‡', 'æ‰©å±•', 'æ€§èƒ½'],
        'è·¨é“¾æŠ€æœ¯': ['cross-chain', 'interoperability', 'è·¨é“¾'],
        'æ•°å­—é’±åŒ…': ['wallet', 'é’±åŒ…'],
        'é¢„è¨€æœº': ['oracle', 'é¢„è¨€æœº'],
        'å»ä¸­å¿ƒåŒ–æ²»ç†': ['governance', 'æ²»ç†'],
        'å»ä¸­å¿ƒåŒ–é‡‘è': ['defi', 'å»ä¸­å¿ƒåŒ–é‡‘è'],
        'NFT': ['nft', 'éåŒè´¨åŒ–ä»£å¸'],
        'Layer2': ['layer 2', 'layer2', 'rollup', 'äºŒå±‚'],
        'æŒ–çŸ¿': ['miner', 'mining', 'æŒ–çŸ¿', 'çŸ¿å·¥'],
        'åŠ å¯†è´§å¸': ['cryptocurrency', 'token', 'ä»£å¸', 'æ•°å­—è´§å¸'],
        'åˆ†å¸ƒå¼ç³»ç»Ÿ': ['distributed', 'åˆ†å¸ƒå¼'],
        'æ•°æ®å­˜å‚¨': ['storage', 'å­˜å‚¨'],
        'ç½‘ç»œåè®®': ['network', 'ç½‘ç»œ'],
        'å¯†ç å­¦': ['cryptographic', 'å¯†ç ', 'å“ˆå¸Œ', 'ç­¾å'],
        'ä»¥å¤ªåŠ': ['ethereum', 'ä»¥å¤ªåŠ'],
        'æ¯”ç‰¹å¸': ['bitcoin', 'æ¯”ç‰¹å¸']
    }
    
    # æŸ¥æ‰¾åŒ¹é…çš„ä¸»é¢˜è¯
    found_topics = []
    for topic, keywords in topics.items():
        for keyword in keywords:
            if keyword in title or keyword in summary:
                found_topics.append(topic)
                break
        if len(found_topics) >= 5:  # æœ€å¤šæå–5ä¸ªä¸»é¢˜
            break
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šä¸»é¢˜ï¼Œåˆ™ä½¿ç”¨é€šç”¨è¯
    if not found_topics:
        found_topics = ['åŒºå—é“¾æŠ€æœ¯']
    
    # ç”Ÿæˆè¯é¢˜æ ‡ç­¾å­—ç¬¦ä¸²
    hashtags = ' '.join([f"#{topic}" for topic in found_topics])
    return hashtags + " #å­¦æœ¯åˆ†äº« #ç§‘æŠ€å‰æ²¿ #AI #è®ºæ–‡æ¨è"


def record_paper_history(paper_info: Dict):
    """è®°å½•è®ºæ–‡å†å²åˆ° markdown æ–‡ä»¶"""
    history_file = "paper_history.md"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    file_exists = os.path.exists(history_file)
    
    with open(history_file, 'a', encoding='utf-8') as f:
        # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡å†™å…¥ï¼Œæ·»åŠ æ ‡é¢˜
        if not file_exists:
            f.write("# åŒºå—é“¾è®ºæ–‡å†å²è®°å½•\n\n")
        
        # æ·»åŠ è®ºæ–‡è®°å½•
        record = f"""## [{paper_info['title']}]({paper_info['link']})
- **æ—¥æœŸ**ï¼š{datetime.now().strftime('%Y-%m-%d')}
- **ä½œè€…**ï¼š{', '.join(paper_info['authors'])}
- **æ‘˜è¦**ï¼š{paper_info['summary'][:200]}...
- **æ¨èç†ç”±**ï¼š{paper_info['recommendation'][:100]}...

---
"""
        f.write(record)


def select_best_paper(papers: List[Dict]) -> Optional[Dict]:
    """ä½¿ç”¨LLMé€‰æ‹©æœ€ä½³è®ºæ–‡"""
    if not papers:
        return None
        
    if len(papers) == 1:
        return papers[0]
    
    # æ„é€ æç¤ºè¯ï¼Œè®©LLMé€‰æ‹©æœ€ä½³è®ºæ–‡
    paper_summaries = []
    for i, paper in enumerate(papers):
        summary = f"""è®ºæ–‡ {i+1}:
æ ‡é¢˜: {paper['title']}
æ‘˜è¦: {paper['summary'][:500]}..."""  # é™åˆ¶æ‘˜è¦é•¿åº¦ä»¥é¿å…è¶…å‡ºä¸Šä¸‹æ–‡çª—å£
        paper_summaries.append(summary)
    
    prompt = f"""
ä½ æ˜¯ä¸€ä½åŒºå—é“¾é¢†åŸŸçš„ä¸“å®¶ï¼Œéœ€è¦ä»ä»¥ä¸‹ {len(papers)} ç¯‡åŒºå—é“¾ç›¸å…³è®ºæ–‡ä¸­é€‰æ‹©æœ€å…·ä»·å€¼å’Œåˆ›æ–°æ€§çš„ä¸€ç¯‡è¿›è¡Œæ·±å…¥è§£è¯»ã€‚
è¯·ç»¼åˆè€ƒè™‘ä»¥ä¸‹å› ç´ è¿›è¡Œé€‰æ‹©ï¼š
1. ç ”ç©¶çš„åˆ›æ–°æ€§å’ŒæŠ€æœ¯æ·±åº¦
2. å¯¹åŒºå—é“¾é¢†åŸŸçš„æ½œåœ¨å½±å“
3. ç ”ç©¶çš„å®Œæ•´æ€§å’Œå®ç”¨æ€§
4. æ˜¯å¦è§£å†³äº†é‡è¦é—®é¢˜

{chr(10).join(paper_summaries)}

è¯·ä»…å›å¤ä½ é€‰æ‹©çš„è®ºæ–‡ç¼–å·ï¼ˆ1-{len(papers)}ï¼‰ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚
""".strip()

    # è°ƒç”¨LLMè·å–é€‰æ‹©ç»“æœ
    answer = call_qwen(prompt)
    if not answer:
        # å¦‚æœè°ƒç”¨å¤±è´¥ï¼Œè¿”å›ç¬¬ä¸€ç¯‡è®ºæ–‡
        return papers[0]
    
    # è§£æé€‰æ‹©ç»“æœ
    try:
        # æå–æ•°å­—
        import re
        numbers = re.findall(r'\d+', answer)
        if numbers:
            selected_index = int(numbers[0])
            if 1 <= selected_index <= len(papers):
                return papers[selected_index - 1]
    except Exception as e:
        print(f"[WARN] è§£æè®ºæ–‡é€‰æ‹©ç»“æœæ—¶å‡ºé”™: {e}")
    
    # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›ç¬¬ä¸€ç¯‡è®ºæ–‡
    return papers[0]


def main():
    print("[START] å¼€å§‹æ‰§è¡Œæ¯æ—¥åŒºå—é“¾è®ºæ–‡æ¨é€ä»»åŠ¡...")
    
    # Step 1: è·å–å€™é€‰è®ºæ–‡
    candidates = get_recent_candidate_papers()
    if not candidates:
        print("[END] è¿‘æœŸæœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å€™é€‰è®ºæ–‡ã€‚")
        with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:
            f.write("# ğŸ“š ArXiv åŒºå—é“¾è®ºæ–‡æ—¥æŠ¥\n\nä»Šæ—¥æš‚æ— æ¨èã€‚\n")
        return

    # Step 2: ç­›é€‰å‡ºä¸åŒºå—é“¾ç›¸å…³çš„è®ºæ–‡ï¼Œæœ€å¤š50ç¯‡
    related_papers = []
    
    # åˆ›å»ºä¸€ä¸ªå€™é€‰è®ºæ–‡çš„å‰¯æœ¬æ¥è¿›è¡Œéšæœºé€‰æ‹©
    candidate_pool = list(candidates)
    # å¦‚æœå€™é€‰è®ºæ–‡æ•°é‡è¶…è¿‡20ç¯‡ï¼Œåˆ™éšæœºé€‰æ‹©20ç¯‡è¿›è¡Œåˆ†æ
    if len(candidate_pool) > 20:
        print(f"[INFO] ä» {len(candidate_pool)} ç¯‡å€™é€‰è®ºæ–‡ä¸­éšæœºé€‰æ‹© 20 ç¯‡è¿›è¡Œåˆ†æ...")
        candidate_pool = random.sample(candidate_pool, 20)
    
    for i, paper in enumerate(candidate_pool):
        if len(related_papers) >= 50:  # æœ€å¤šåªéœ€è¦50ç¯‡
            break
            
        print(f"[PROCESS] æ­£åœ¨åˆ†æç¬¬ {i+1}/{len(candidate_pool)} ç¯‡å€™é€‰è®ºæ–‡: {paper['title']}...")
        
        # é¦–å…ˆåˆ¤æ–­æ˜¯å¦ä¸åŒºå—é“¾ç›¸å…³
        if is_blockchain_related(paper['title'], paper['summary']):
            related_papers.append(paper)
            print(f"[SELECT] âœ… æ‰¾åˆ°ç›¸å…³è®ºæ–‡ ({len(related_papers)}/50): {paper['title']}... é“¾æ¥: {paper['link']}")
            
        time.sleep(1)  # ç¤¼è²Œç­‰å¾…ï¼Œé¿å…é¢‘ç¹è¯·æ±‚
        
    if not related_papers:
        print("[END] ç»è¿‡ç­›é€‰ï¼Œæœªå‘ç°å®Œå…¨ç¬¦åˆ'åŒºå—é“¾'ä¸»é¢˜çš„è®ºæ–‡ã€‚")
        with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:
            f.write("# ğŸ“š ArXiv åŒºå—é“¾è®ºæ–‡æ—¥æŠ¥\n\nä»Šæ—¥æš‚æ— æ¯”é€‰ä¸­çš„åŒºå—é“¾è®ºæ–‡ã€‚\n")
        return

    print(f"[INFO] å…±æ‰¾åˆ° {len(related_papers)} ç¯‡åŒºå—é“¾ç›¸å…³è®ºæ–‡ï¼Œå¼€å§‹é€‰æ‹©æœ€ä¼˜è®ºæ–‡...")
    
    # Step 3: ä½¿ç”¨LLMä»ç›¸å…³è®ºæ–‡ä¸­é€‰æ‹©1ç¯‡æœ€ä¼˜è®ºæ–‡è¿›è¡Œç²¾è¯»
    selected_paper = select_best_paper(related_papers)
    if selected_paper:
        print(f"[SELECT] âœ… é€‰æ‹©æœ€ä¼˜è®ºæ–‡: {selected_paper['title'][:50]}...")
    else:
        print("[ERROR] è®ºæ–‡é€‰æ‹©å¤±è´¥")
        return
    
    # è·å–è®ºæ–‡è¯¦ç»†ä¿¡æ¯
    published_venue = selected_paper['comment'] if selected_paper['comment'] else ""
    if published_venue and is_ccf_a_venue(published_venue):
        print(f"[INFO] è®ºæ–‡å‘è¡¨åœ¨ CCF-A ç±»ä¼šè®®/æœŸåˆŠ: {published_venue}")
    
    details = generate_summary_and_insights(selected_paper['title'], selected_paper['summary'], selected_paper['link'])
    
    # å°è¯•æå–ç¬¬ä¸€å•ä½ä¿¡æ¯
    affiliation = ""
    if 'affiliation' in selected_paper and selected_paper['affiliation']:
        affiliation = selected_paper['affiliation']
    elif 'authors' in selected_paper and len(selected_paper['authors']) > 0:
        # ç®€å•å¤„ç†ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªä½œè€…ä½œä¸ºç¤ºä¾‹
        affiliation = "æœªçŸ¥å•ä½"  # å®é™…é¡¹ç›®ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„é€»è¾‘æå–å•ä½ä¿¡æ¯
    
    final_paper_info = {
        "title": selected_paper['title'],
        "authors": selected_paper['authors'],
        "link": selected_paper['link'],
        "published": selected_paper['published'],
        "summary": details["summary"],
        "insights": details["insights"],
        "recommendation": details["recommendation"],
        "venue": published_venue,
        "affiliation": affiliation
    }

    # Step 4: æ ¼å¼åŒ–å¹¶ä¿å­˜ç»“æœ
    final_content = format_output(final_paper_info)
    
    with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:
        f.write(final_content)
    
    # Step 5: è®°å½•å†å²è®ºæ–‡
    record_paper_history(final_paper_info)
    
    # Step 6: ç”Ÿæˆå°çº¢ä¹¦é£æ ¼çš„å†…å®¹
    xiaohongshu_content = format_xiaohongshu_output(final_paper_info)
    with open("xiaohongshu_post.md", 'w', encoding='utf-8') as f:
        f.write(xiaohongshu_content)
    
    # Step 7: ç”Ÿæˆå°çº¢ä¹¦å°é¢æ–‡å­—ä¿¡æ¯
    xiaohongshu_cover = generate_xiaohongshu_cover_text(final_paper_info)
    with open("xiaohongshu_cover.txt", 'w', encoding='utf-8') as f:
        f.write(xiaohongshu_cover)
    
    print(f"[SUCCESS] å·²æˆåŠŸç”ŸæˆæŠ¥å‘Šå¹¶ä¿å­˜è‡³ '{OUTPUT_FILENAME}'")
    print(f"[SUCCESS] å·²è®°å½•è®ºæ–‡å†å²åˆ° 'paper_history.md'")
    print(f"[SUCCESS] å·²ç”Ÿæˆå°çº¢ä¹¦é£æ ¼å†…å®¹å¹¶ä¿å­˜è‡³ 'xiaohongshu_post.md'")
    print(f"[SUCCESS] å·²ç”Ÿæˆå°çº¢ä¹¦å°é¢æ–‡å­—å¹¶ä¿å­˜è‡³ 'xiaohongshu_cover.txt'")


def get_paper_by_id(paper_id: str) -> Optional[Dict]:
    """é€šè¿‡è®ºæ–‡IDè·å–è®ºæ–‡ä¿¡æ¯"""
    try:
        # å¤„ç†å®Œæ•´é“¾æ¥çš„æƒ…å†µï¼Œæå–ID
        if paper_id.startswith("http"):
            # ä»é“¾æ¥ä¸­æå–IDï¼Œä¾‹å¦‚ä» http://arxiv.org/abs/2510.03697v1 æå– 2510.03697
            import re
            match = re.search(r'/(\d+\.\d+)(v\d+)?$', paper_id)
            if match:
                paper_id = match.group(1)
            else:
                print(f"[ERROR] æ— æ³•ä»é“¾æ¥ {paper_id} ä¸­æå–è®ºæ–‡ID")
                return None
        
        search = arxiv.Search(id_list=[paper_id])
        results = list(search.results())
        if results:
            paper = results[0]
            return {
                'title': paper.title,
                'authors': [author.name for author in paper.authors],
                'summary': paper.summary,
                'link': paper.entry_id,
                'published': paper.published,
                'comment': paper.comment if paper.comment else ""
            }
        else:
            print(f"[ERROR] æœªæ‰¾åˆ°IDä¸º {paper_id} çš„è®ºæ–‡")
            return None
    except Exception as e:
        print(f"[ERROR] è·å–è®ºæ–‡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None


def generate_report_from_arxiv_id(paper_id: str):
    """é€šè¿‡ArXiv IDç”Ÿæˆè®ºæ–‡æ—¥æŠ¥"""
    print(f"[START] å¼€å§‹å¤„ç†è®ºæ–‡ ID: {paper_id}")
    
    # åˆ›å»ºå•ç‹¬çš„æ–‡ä»¶å¤¹æ¥ä¿å­˜å•ç¯‡è®ºæ–‡åˆ†æç»“æœ
    single_paper_dir = "single_paper_reports"
    os.makedirs(single_paper_dir, exist_ok=True)
    
    # è·å–è®ºæ–‡ä¿¡æ¯
    paper_info = get_paper_by_id(paper_id)
    if not paper_info:
        print("[ERROR] æ— æ³•è·å–è®ºæ–‡ä¿¡æ¯")
        return
    
    print(f"[INFO] æˆåŠŸè·å–è®ºæ–‡: {paper_info['title']}")
    print(f"[INFO] è®ºæ–‡é“¾æ¥: {paper_info['link']}")
    
    # ç”Ÿæˆè®ºæ–‡æ‘˜è¦å’Œå…³é”®ç‚¹
    details = generate_summary_and_insights(paper_info['title'], paper_info['summary'], paper_info['link'])
    
    # è·å–å‘è¡¨ä¿¡æ¯
    published_venue = paper_info['comment'] if paper_info['comment'] else ""
    if published_venue and is_ccf_a_venue(published_venue):
        print(f"[INFO] è®ºæ–‡å‘è¡¨åœ¨ CCF-A ç±»ä¼šè®®/æœŸåˆŠ: {published_venue}")
    
    # å°è¯•æå–ç¬¬ä¸€å•ä½ä¿¡æ¯
    affiliation = ""
    if 'affiliation' in paper_info and paper_info['affiliation']:
        affiliation = paper_info['affiliation']
    elif 'authors' in paper_info and len(paper_info['authors']) > 0:
        # ç®€å•å¤„ç†ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªä½œè€…ä½œä¸ºç¤ºä¾‹
        affiliation = "æœªçŸ¥å•ä½"  # å®é™…é¡¹ç›®ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„é€»è¾‘æå–å•ä½ä¿¡æ¯
    
    final_paper_info = {
        "title": paper_info['title'],
        "authors": paper_info['authors'],
        "link": paper_info['link'],
        "published": paper_info['published'],
        "summary": details["summary"],
        "insights": details["insights"],
        "recommendation": details["recommendation"],
        "venue": published_venue,
        "affiliation": affiliation
    }

    # æ ¼å¼åŒ–å¹¶ä¿å­˜ç»“æœ
    final_content = format_output(final_paper_info)
    
    # å¤„ç†æ–‡ä»¶åï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„IDï¼ˆå»é™¤ç‰ˆæœ¬å·ç­‰ï¼‰
    clean_paper_id = paper_id
    if paper_id.startswith("http"):
        import re
        match = re.search(r'/(\d+\.\d+)(v\d+)?$', paper_id)
        if match:
            clean_paper_id = match.group(1)
    
    # ä¿å­˜åˆ°å•ç‹¬çš„æ–‡ä»¶å¤¹ä¸­
    output_filename = f"{single_paper_dir}/paper_{clean_paper_id}.md"
    with open(output_filename, 'w', encoding='utf-8') as f:
        f.write(final_content)
    
    # ç”Ÿæˆå°çº¢ä¹¦é£æ ¼çš„å†…å®¹
    xiaohongshu_content = format_xiaohongshu_output(final_paper_info)
    xiaohongshu_filename = f"{single_paper_dir}/paper_{clean_paper_id}_xiaohongshu.md"
    with open(xiaohongshu_filename, 'w', encoding='utf-8') as f:
        f.write(xiaohongshu_content)
    
    # ç”Ÿæˆå°çº¢ä¹¦å°é¢æ–‡å­—ä¿¡æ¯
    xiaohongshu_cover = generate_xiaohongshu_cover_text(final_paper_info)
    xiaohongshu_cover_filename = f"{single_paper_dir}/paper_{clean_paper_id}_cover.txt"
    with open(xiaohongshu_cover_filename, 'w', encoding='utf-8') as f:
        f.write(xiaohongshu_cover)
    
    print(f"[SUCCESS] å·²æˆåŠŸç”ŸæˆæŠ¥å‘Šå¹¶ä¿å­˜è‡³ '{output_filename}'")
    print(f"[SUCCESS] å·²ç”Ÿæˆå°çº¢ä¹¦é£æ ¼å†…å®¹å¹¶ä¿å­˜è‡³ '{xiaohongshu_filename}'")
    print(f"[SUCCESS] å·²ç”Ÿæˆå°çº¢ä¹¦å°é¢æ–‡å­—å¹¶ä¿å­˜è‡³ '{xiaohongshu_cover_filename}'")
    print(f"[INFO] è®ºæ–‡é“¾æ¥: {paper_info['link']}")

def generate_xiaohongshu_cover_text(paper_info: Dict):
    """ç”Ÿæˆå°çº¢ä¹¦é£æ ¼çš„å°é¢æ–‡å­—ä¿¡æ¯ï¼ŒåŸºäºå®é™…è®ºæ–‡å†…å®¹"""
    # æ ¹æ®è®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦æå–å…³é”®è¯ä½œä¸ºä¸»è¦å†…å®¹æ€»ç»“
    title = paper_info['title'].lower()
    summary = paper_info['summary'].lower()
    
    # å®šä¹‰ä¸€äº›å¸¸è§çš„åŒºå—é“¾ç›¸å…³ä¸»é¢˜è¯
    topics = {
        'å…±è¯†': ['consensus', 'å…±è¯†', 'bft', 'æ‹œå åº­', 'pbft'],
        'æ™ºèƒ½åˆçº¦': ['smart contract', 'æ™ºèƒ½åˆçº¦', 'solidity'],
        'å®‰å…¨': ['security', 'attack', 'å®‰å…¨', 'æ”»å‡»', 'é˜²æŠ¤'],
        'éšç§': ['privacy', 'åŒ¿å', 'é›¶çŸ¥è¯†', 'zk', 'private'],
        'æ€§èƒ½': ['performance', 'scalability', 'sharding', 'åˆ†ç‰‡', 'æ‰©å±•', 'æ€§èƒ½'],
        'è·¨é“¾': ['cross-chain', 'interoperability', 'è·¨é“¾'],
        'é’±åŒ…': ['wallet', 'é’±åŒ…'],
        'é¢„è¨€æœº': ['oracle', 'é¢„è¨€æœº'],
        'æ²»ç†': ['governance', 'æ²»ç†'],
        'defi': ['defi', 'å»ä¸­å¿ƒåŒ–é‡‘è'],
        'nft': ['nft', 'éåŒè´¨åŒ–ä»£å¸'],
        'layer2': ['layer 2', 'layer2', 'rollup', 'äºŒå±‚'],
        'çŸ¿å·¥': ['miner', 'mining', 'æŒ–çŸ¿', 'çŸ¿å·¥'],
        'æ•°å­—è´§å¸': ['cryptocurrency', 'token', 'ä»£å¸', 'æ•°å­—è´§å¸'],
        'åˆ†å¸ƒå¼': ['distributed', 'åˆ†å¸ƒå¼'],
        'å­˜å‚¨': ['storage', 'å­˜å‚¨'],
        'ç½‘ç»œ': ['network', 'ç½‘ç»œ']
    }
    
    # æŸ¥æ‰¾åŒ¹é…çš„ä¸»é¢˜è¯
    found_topics = []
    for topic, keywords in topics.items():
        for keyword in keywords:
            if keyword in title or keyword in summary:
                found_topics.append(topic)
                break
        if len(found_topics) >= 2:  # æœ€å¤šæå–2ä¸ªä¸»é¢˜
            break
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šä¸»é¢˜ï¼Œåˆ™ä½¿ç”¨é€šç”¨è¯
    if not found_topics:
        found_topics = ['åŒºå—é“¾æŠ€æœ¯']
    
    # ç”Ÿæˆä¸€å¥è¯å°é¢æ–‡å­—ï¼Œ30å­—ä»¥å†…
    main_content = ' & '.join(found_topics)
    cover_text = f"ArXivåŒºå—é“¾è®ºæ–‡åˆ†äº« {main_content}"
    
    return cover_text

# æ·»åŠ å®šæ—¶ä»»åŠ¡æ”¯æŒ
def schedule_daily_task():
    """è®¾ç½®æ¯æ—¥å®šæ—¶ä»»åŠ¡"""
    import schedule
    
    # å®‰æ’æ¯æ—¥ä»»åŠ¡
    schedule.every().day.at(SCHEDULE_TIME).do(main)
    
    print(f"[SCHEDULE] å·²è®¾ç½®æ¯æ—¥å®šæ—¶ä»»åŠ¡ï¼Œå°†åœ¨æ¯å¤© {SCHEDULE_TIME} æ‰§è¡Œ")
    
    # ä¿æŒç¨‹åºè¿è¡Œ
    while True:
        schedule.run_pending()
        time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡


if __name__ == "__main__":
    # æ£€æŸ¥æ˜¯å¦éœ€è¦å®šæ—¶æ‰§è¡Œ
    import sys
    if len(sys.argv) > 1:
        if sys.argv[1] == "--schedule":
            schedule_daily_task()
        elif sys.argv[1] == "--arxiv-id" and len(sys.argv) > 2:
            # é€šè¿‡ArXiv IDç”ŸæˆæŠ¥å‘Š
            paper_id = sys.argv[2]
            generate_report_from_arxiv_id(paper_id)
        else:
            print("ç”¨æ³•:")
            print("  python blockchain_paper_daily.py                     # æ‰§è¡Œæ¯æ—¥è®ºæ–‡ç­›é€‰")
            print("  python blockchain_paper_daily.py --schedule         # å®šæ—¶æ‰§è¡Œæ¯æ—¥è®ºæ–‡ç­›é€‰")
            print("  python blockchain_paper_daily.py --arxiv-id <ID>    # é€šè¿‡ArXiv IDç”Ÿæˆè®ºæ–‡æŠ¥å‘Š")
    else:
        main()